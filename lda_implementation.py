# -*- coding: utf-8 -*-
"""LDA_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zO8XovGQg58Y2LKpZRZ_MchWrgO--Nfi
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/



import warnings
warnings.filterwarnings("ignore")

paper = pd.read_csv('2020-merged.csv')
paper.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1'] ,inplace = True)

"""# New Section"""

paper.drop(columns = [ 'Doc_number' ,'Type', 'App_Date' , 'Country' , 'Org_Name' , 'Org_City' , 'Org_Country' , 'No_inventors'] ,inplace = True)

paper.head()

import re

paper['paper_text_processed'] = \
paper['Title'].map(lambda x: re.sub('[,\.!?]', '', x))


paper['paper_text_processed'] = \
paper['paper_text_processed'].map(lambda x: x.lower())


paper['paper_text_processed'].head()

from wordcloud import WordCloud


long_string = ','.join(list(paper['paper_text_processed'].values))

wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')


wordcloud.generate(long_string)

wordcloud.to_image()

import nltk
nltk.download('wordnet')

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
import string

stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use' , 'first' , 'second' , 'may' , 'data' , 'memeber', 'display' , 'include' , 
                   'includes', 'method', 'display' , 'one' , 'unit', 'third', 'end' , 'side' , 'least' , 'configured', 'part', 
                   'said' , 'least' , 'comprising' , 'body' , 'position' , 'support' , 'user'])

def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
        
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

def lemma(texts):
  lemma = WordNetLemmatizer() 
  return [[lemma.lemmatize(word) for word in simple_preprocess(str(doc))] for doc in texts]

data = paper.paper_text_processed.values.tolist()
data_words = list(sent_to_words(data))

# remove stop words
data_words = remove_stopwords(data_words)

# data_words = lemma(data_words)

print(data_words)
print(data_words[:1][0][:30])

nltk.download('omw-1.4')

def lemma(texts):
  lemma = WordNetLemmatizer() 
  return [[lemma.lemmatize(word) for word in simple_preprocess(str(doc))] for doc in texts]

data_words = lemma(data_words)

# identifying term document frequency. First we create a dictionary and corpus , then calculate
import gensim.corpora as corpora


id2word = corpora.Dictionary(data_words)


texts = data_words

corpus = [id2word.doc2bow(text) for text in texts]


print(corpus[:1][0][:30])

from pprint import pprint

num_topics = 10


lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)

pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

!pip install pyLDAvis

!pip install pyLDAvis.gensim

import pyLDAvis
# import pyLDAvis.gensim_models
import pyLDAvis.gensim_models
import pickle 
import pyLDAvis
import os

# os.chdir('..')


pyLDAvis.enable_notebook()
# LDAvis_data_filepath = '/Users/malaikagupta/Desktop/Fall 2022/Capstone/Data/results/ldavis_prepared_20'
LDAvis_data_filepath = 'ldavis_prepared_20'

print(LDAvis_data_filepath)

# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself
LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
with open(LDAvis_data_filepath, 'wb') as f:
    pickle.dump(LDAvis_prepared, f)
        
# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)
pyLDAvis.save_html(LDAvis_prepared, 'ldavis_prepared_20' +'.html')
LDAvis_prepared

