{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffe71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shreya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import shutil\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa86a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f925d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    #print(\"Tag is here \",nltk_tag)\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2208e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_generator(partition):\n",
    "    countries = []\n",
    "    doc_numbers = []\n",
    "    dates=[]\n",
    "    utilities = []\n",
    "    \n",
    "    titles=[]\n",
    "    org_names=[]\n",
    "    org_cities=[]\n",
    "    org_countries=[]\n",
    "    nois = []\n",
    "    abstracts=[]\n",
    "    word_counts = []\n",
    "    \n",
    "    files = os.listdir('Unzipped/{}'.format(partition))\n",
    "    for file in files: \n",
    "        word_list = []\n",
    "        with open(f'Unzipped/{partition}/{file}') as f:\n",
    "            try:\n",
    "                dic=xmltodict.parse(f.read())\n",
    "                countries.append(dic['us-patent-application']['us-bibliographic-data-application']['application-reference']['document-id']['country'])\n",
    "                doc_numbers.append(dic['us-patent-application']['us-bibliographic-data-application']['application-reference']['document-id']['doc-number'])\n",
    "                dates.append(dic['us-patent-application']['us-bibliographic-data-application']['application-reference']['document-id']['date'])\n",
    "                utilities.append(dic['us-patent-application']['us-bibliographic-data-application']['application-reference']['@appl-type'])\n",
    "                titles.append(dic['us-patent-application']['us-bibliographic-data-application']['invention-title']['#text'])\n",
    "                try:\n",
    "                    org_names.append(dic['us-patent-application']['us-bibliographic-data-application']['us-parties']['us-applicants']['us-applicant']['addressbook']['orgname'])\n",
    "                except:\n",
    "                    org_names.append(np.nan)\n",
    "                try:\n",
    "                    org_cities.append(dic['us-patent-application']['us-bibliographic-data-application']['us-parties']['us-applicants']['us-applicant']['addressbook']['address']['city'])\n",
    "                except:\n",
    "                    org_cities.append(np.nan)\n",
    "                try:    \n",
    "                    org_countries.append(dic['us-patent-application']['us-bibliographic-data-application']['us-parties']['us-applicants']['us-applicant']['addressbook']['address']['country'])\n",
    "                except:\n",
    "                    org_countries.append(np.nan)\n",
    "\n",
    "                nois.append(len(dic['us-patent-application']['us-bibliographic-data-application']['us-parties']['inventors']['inventor']))\n",
    "                abstracts.append(dic['us-patent-application']['abstract']['p']['#text'])\n",
    "                \n",
    "                for i in range(len(dic['us-patent-application']['claims']['claim'])):\n",
    "                    try:\n",
    "                        text = dic['us-patent-application']['claims']['claim'][i]['claim-text']['#text']\n",
    "                        \n",
    "                        all_words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "\n",
    "                        pos_tagged = nltk.pos_tag(all_words) \n",
    "                        \n",
    "                        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "                        \n",
    "                        \n",
    "                        lemmatized_sentence = []\n",
    "                        for word, tag in wordnet_tagged:\n",
    "                            if tag is None:\n",
    "                                # if there is no available tag, append the token as is\n",
    "                                lemmatized_sentence.append(word)\n",
    "                            else:       \n",
    "                                # else use the tag to lemmatize the token\n",
    "                                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "                        \n",
    "                        filtered_words = [word.lower() for word in lemmatized_sentence if word.lower() not in stop]\n",
    "                        word_list.extend(filtered_words)\n",
    "                    except:\n",
    "                        continue\n",
    "                word_counts.append(Counter(word_list).items())\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    df = pd.DataFrame(list(zip(doc_numbers, titles, utilities, dates, countries, org_names, org_cities, org_countries, nois, abstracts, word_counts)),\n",
    "               columns =['Doc_number', 'Title', 'Type','App_Date','Country','Org_Name','Org_City','Org_Country','No_inventors','Abstract','Description'])\n",
    "    \n",
    "    df['App_Date'] = pd.to_datetime(df['App_Date'])\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce55ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id  = 'AKIA4DCDKSPQB6DM4TWS'\n",
    "secret_access_key = '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'\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "resources = session.resource('s3')\n",
    "\n",
    "s3 = session.client('s3')\n",
    "my_bucket = resources.Bucket('capstone-storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d14fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket='capstone-storage')\n",
    "files = response.get(\"Contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f2ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = []\n",
    "\n",
    "for file in files:\n",
    "    if file['Key'].endswith('.zip'):\n",
    "        locs.append(file['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5e7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_locs = []\n",
    "for loc in locs:\n",
    "    file_name = loc.split('/')[2]\n",
    "    if len(file_name.split('_'))>1 and file_name.endswith('.zip'):\n",
    "        updated_locs.append(file_name.split('_')[0]+'.zip')\n",
    "    elif file_name.endswith('.zip'):\n",
    "        updated_locs.append(file_name)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc,uloc in zip(locs,updated_locs):\n",
    "    year = loc.split('/')[1]\n",
    "    if int(year)!=2019:\n",
    "        continue\n",
    "    out_file = loc.split('/')[2] #has zip tag\n",
    "    s3.download_file('capstone-storage',loc,Filename='unzipped/'+out_file)\n",
    "    os.system('unzip unzipped/{} -d unzipped'.format(out_file))\n",
    "    os.system('python3 parse_patents.py -i Unzipped/{}.xml'.format(uloc[:-4]))\n",
    "    os.remove('unzipped/{}.xml'.format(uloc[:-4]))\n",
    "    os.remove('unzipped/{}'.format(out_file))\n",
    "    df = dataframe_generator(uloc[:-4])\n",
    "    shutil.rmtree('Unzipped/{}'.format(uloc[:-4]))\n",
    "    df.to_csv('unzipped/{}.csv'.format(uloc[:-4]))\n",
    "    my_bucket.upload_file('unzipped/{}.csv'.format(uloc[:-4]), 'cleaned_data_with_description/{}/{}.csv'.format(year,uloc[:-4]))\n",
    "    os.remove('unzipped/{}.csv'.format(uloc[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441b597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f5b49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
